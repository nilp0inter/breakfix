
                        <!DOCTYPE html>
                        <html lang="en">
                        <head>
                            <meta charset="UTF-8">
                            <meta name="viewport" content="width=device-width, initial-scale=1.0">
							<style>
								body {
									background-color: white; /* Ensure the iframe has a white background */
								}

								
							</style>
                        </head>
                        <body>
                            <!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Agentic TDD Foundry: System Specification</title>
</head>
<body>

    <h1>Agentic TDD Foundry: System Specification</h1>

    <p>This document details the architecture and operational flow of the Agentic TDD Foundry, a multi-agent system designed to transform high-level user ideas into mathematically verified, production-grade software. The system employs a "Refinement & Reconstruction" paradigm, leveraging LLMs for creative problem-solving and rigorous mechanistic checks for quality assurance.</p>

    <p>The system is structured as a series of nested control loops, each addressing a specific scope (Project, Unit, Atomic) and ensuring quality through iterative feedback. Agents implement specific roles, utilizing Pydantic AI for user-facing interactions and Anthropic Agent SDK (Claude Code) for code generation tasks.</p>

    <h2>1. System Architecture Overview</h2>

    <p>The system operates through nine primary phases, organized by their execution cardinality and scope:</p>
    <ol>
        <li><strong>Specification Loop:</strong> (Project Scope) Captures user intent into a formal specification and project metadata.</li>
        <li><strong>E2E Test Builder:</strong> (Project Scope) Creates test harness using Claude Code.</li>
        <li><strong>Scaffolding:</strong> (Project Scope) Initializes Python project structure using PyScaffold.</li>
        <li><strong>Prototyping Loop:</strong> (Project Scope) Generates a working but unrefined prototype.</li>
        <li><strong>Refinement Loop:</strong> (Project Scope) Applies architectural separation (Functional Core/Imperative Shell).</li>
        <li><strong>Distillation Loop:</strong> (Project Scope) Decomposes the refined prototype into atomic units.</li>
        <li><strong>Oracle Generator:</strong> (Unit Scope) Analyzes each unit and generates exhaustive test case specifications.</li>
        <li><strong>Ratchet Cycle:</strong> (Atomic Scope) Builds individual units using strict TDD, once per test case.</li>
        <li><strong>Crucible Cycle:</strong> (Unit Scope) Hardens and optimizes each unit after its construction.</li>
    </ol>

    <h3>1.1. Execution Flow Diagram</h3>
    <pre class="mermaid">
        graph TD  
            %% Styling  
            classDef scope fill:#eceff1,stroke:#455a64,stroke-width:2px,stroke-dasharray: 5 5;  
            classDef agent fill:#e1f5fe,stroke:#01579b,stroke-width:2px;  
            classDef check fill:#fff9c4,stroke:#fbc02d,stroke-width:2px;  
            classDef artifact fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px;  
            classDef process fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px;  
            classDef ratchet fill:#ffebee,stroke:#c62828,stroke-width:4px;  
            classDef error fill:#ffebee,stroke:#c62828,stroke-width:1px,stroke-dasharray: 2 2;  

            UserIdea(("User Idea")) --> SpecEntry  

            %% =========================================================  
            %% SCOPE: PROJECT
            %% =========================================================  
            subgraph ProjectScope ["Execution Scope: PROJECT - Once per Idea"]
                direction TB
                SpecEntry(Start Phase 1) --> Analyst["Agent: Analyst (Pydantic AI)"]:::agent
                Analyst --> CheckSpec{"Check: Consistency"}:::check
                CheckSpec -- "Error" --> Analyst
                CheckSpec -- OK --> ReqArtifact["Artifact: Spec + Fixtures"]:::artifact
                ReqArtifact --> E2EEntry(Start Phase 1a)
                E2EEntry --> E2EBuilder["Agent: E2E Builder (Claude Code)"]:::agent
                E2EBuilder --> CheckE2EBuilder{"Check: Tests Pass?"}:::check
                CheckE2EBuilder -- "Fail" --> E2EBuilder
                CheckE2EBuilder -- OK --> InterfaceAnalyzer["Agent: Interface Analyzer"]:::agent
                InterfaceAnalyzer --> E2EArtifact["Artifact: e2e-tests"]:::artifact
                E2EArtifact --> ScaffoldEntry(Start Phase 1b)
                ScaffoldEntry --> Scaffold["Process: PyScaffold putup"]:::process
                Scaffold --> CheckScaffold{"Check: Success?"}:::check
                CheckScaffold -- "Error" --> ScaffoldError["Error: Halt"]:::error
                CheckScaffold -- OK --> ScaffoldArtifact["Artifact: prototype/"]:::artifact
                ScaffoldArtifact --> ProtoEntry(Start Phase 2)
                ProtoEntry --> Hacker["Agent: Prototyper"]:::agent
                Hacker --> CheckE2E{"Check: E2E Tests"}:::check
                CheckE2E -- "Fail" --> Hacker
                CheckE2E -- OK --> ProtoArtifact["Artifact: Working Prototype"]:::artifact
                ProtoArtifact --> RefineEntry(Start Phase 3)
                RefineEntry --> Refactorer["Agent: Architect"]:::agent
                Refactorer --> CheckArch{"Check: Taint"}:::check
                CheckArch -- "Error" --> Refactorer
                CheckArch -- OK --> ArchArtifact["Artifact: Refined Tree"]:::artifact
                ArchArtifact --> DistillEntry(Start Phase 4)
                DistillEntry --> DepGraph["Process: pydeps"]:::process
                DepGraph --> ASTAnalysis["Process: AST Analysis"]:::process
                ASTAnalysis --> TopoSort["Process: Topological Sort"]:::process
                TopoSort --> UnitList["Artifact: Unit Queue"]:::artifact
                UnitList --> WorkspaceCopy["Process: Copy to Production"]:::process
                WorkspaceCopy --> ProductionArtifact["Artifact: production/"]:::artifact
            end

            ProductionArtifact --> UnitIterator{"Next Unit?"}:::process

            %% =========================================================  
            %% PHASE 4.5: Oracle
            %% =========================================================  
            subgraph OracleScope ["Phase 4.5: Oracle Generator"]
                direction TB
                UnitIterator -- Yes --> OracleEntry(Start Oracle Generation)
                OracleEntry --> Oracle["Agent: Oracle (Pydantic AI)"]:::agent
                Oracle --> OracleArtifact["Artifact: Test Specs"]:::artifact
            end

            OracleArtifact --> TestIterator{"Next Test Case?"}:::process

            %% =========================================================  
            %% SCOPE: UNIT
            %% =========================================================  
            subgraph UnitScope ["Execution Scope: UNIT - Once per Unit"]
                direction TB
                subgraph AtomicScope ["Phase 5: The Ratchet Cycle"]
                    direction TB
                    TestIterator -- Yes --> RatchetStart((Start Test)):::ratchet
                    RatchetStart --> RedAgent["Agent: Tester"]:::agent
                    RedAgent --> Validator["Agent: Test Validator"]:::agent
                    Validator --> CheckValidator{"Valid?"}:::check
                    CheckValidator -- "No" --> RedAgent
                    CheckValidator -- OK --> CheckRed{"Check State"}:::check
                    CheckRed -- "Error" --> RedAgent
                    CheckRed -- "Pass unexpectedly" --> Arbiter{"Agent: Arbiter"}:::agent
                    Arbiter -- "Discard" --> RewindRed["Process: Rewind"]:::process
                    Arbiter -- "Keep" --> Commit
                    CheckRed -- OK --> GreenAgent["Agent: Developer"]:::agent
                    GreenAgent --> CheckGreen{"Pass?"}:::check
                    CheckGreen -- "Fail" --> GreenAgent
                    CheckGreen -- OK --> Commit["Process: Commit"]:::process
                end

                RewindRed --> TestIterator
                Commit --> TestIterator

                TestIterator -- "No" --> CrucibleStart(Start Phase 7)
                  
                subgraph CruciblePhase ["Phase 7: The Crucible"]
                    direction TB
                    CrucibleStart --> Mutator["Tool: Mutmut"]:::process
                    Mutator --> CheckMut{"Mutants?"}:::check
                    CheckMut -- Yes --> Sentinel["Agent: Sentinel"]:::agent
                    Sentinel --> CheckKill{"Killed?"}:::check
                    CheckKill -- "No" --> Sentinel
                    CheckKill -- OK --> AddTest["Process: Add Test"]:::process
                    AddTest --> Mutator
                    CheckMut -- No --> Optimizer["Agent: Optimizer"]:::agent
                    Optimizer --> CheckRegress{"Regression?"}:::check
                    CheckRegress -- "Yes" --> Optimizer
                    CheckRegress -- OK --> Converged{"Done?"}:::check
                    Converged -- No --> Optimizer
                end
                Converged -- Yes --> FinalUnit["Artifact: Final Unit"]:::artifact
            end

            FinalUnit --> UnitIterator
            UnitIterator -- No --> Production["Final Production System"]:::artifact
    </pre>

    <h2>2. Agent Roles and Technologies</h2>
    <p>This system design specifies the use of different LLM technologies adapted to the specific task at hand:</p>
    <ul>
        <li><strong>Pydantic AI:</strong> Used for agents interacting directly with the user (e.g., clarifying specs), where structured output and high-quality natural language interaction are paramount.</li>
        <li><strong>Anthropic Agent SDK (Claude Code):</strong> Used for code generation, test writing, and code refactoring tasks, leveraging its strong capabilities in code understanding and generation.</li>
    </ul>

    <h2>3. Phase Details by Execution Scope</h2>

    <h3>3.1. Project-Level Scope (1x per Idea)</h3>
    <p>This linear sequence executes once per user idea, progressively refining ambiguity into a concrete plan.</p>

    <h4>Phase 1: The Specification Loop</h4>
    <ul>
        <li><strong>Cardinality:</strong> Runs <strong>once per project idea</strong>.</li>
        <li><strong>Agent:</strong> <code>Analyst</code> (Pydantic AI).</li>
        <li><strong>Goal:</strong> Converge on a clear, testable software specification with concrete examples and collect project metadata for scaffolding.</li>
        <li><strong>Input:</strong> Raw user idea from the input harness.</li>
        <li><strong>Process:</strong> The <code>Analyst</code> conducts an interactive Q&A session with the user via the <code>ask_user</code> tool, following a top-down tree descent interview strategy:
            <ol>
                <li><strong>Broad Understanding:</strong> Core problem, target users, main goal</li>
                <li><strong>Project Identity:</strong> Project name, package name, description, license, URL, and CI preferences</li>
                <li><strong>Feature Exploration:</strong> Key features and expected behaviors</li>
                <li><strong>Edge Cases:</strong> Boundary conditions and unusual inputs</li>
                <li><strong>Error Handling:</strong> Failure scenarios and recovery strategies</li>
                <li><strong>Validation:</strong> Confirm critical assumptions before finalizing</li>
            </ol>
            The agent uses Pydantic AI's <code>iter()</code> context manager to maintain conversation context across multiple Q&A exchanges (typically 5-8).
        </li>
        <li><strong>Validation:</strong> Pydantic AI automatically validates the structured output via the <code>AnalystOutput</code> model:
            <ul>
                <li>Specification must be at least 100 characters</li>
                <li>At least 3 <code>TestFixture</code> objects required (happy path, edge case, error case)</li>
            </ul>
        </li>
        <li><strong>Output:</strong> <code>AnalystOutput</code> containing a specification string, a list of <code>TestFixture</code> objects (each with name, description, input_data, and expected_output), and <code>ProjectMetadata</code> for scaffolding.</li>
    </ul>

    <h4>Phase 1a: E2E Test Builder</h4>
    <ul>
        <li><strong>Cardinality:</strong> Runs <strong>once per project</strong>, after specification.</li>
        <li><strong>Agent:</strong> Claude Code via <code>claude-agent-sdk</code>.</li>
        <li><strong>Goal:</strong> Create a test harness that can validate prototypes against fixtures.</li>
        <li><strong>Input:</strong> Specification and test fixtures from Phase 1.</li>
        <li><strong>Process:</strong>
            <ol>
                <li>Create <code>e2e-tests/</code> directory</li>
                <li>Write <code>fixtures.json</code> with test fixtures</li>
                <li>Use Claude Code to generate <code>run_tests.py</code> that:
                    <ul>
                        <li>Reads fixtures from JSON</li>
                        <li>Accepts program path as CLI argument</li>
                        <li>Interfaces with program (stdin/stdout, HTTP, CLI args, etc.)</li>
                        <li>Compares results with expected outputs</li>
                        <li>Reports pass/fail with exit code</li>
                    </ul>
                </li>
                <li>Generate <code>mock_program.py</code> that implements fixture behavior for self-verification</li>
                <li>Verify all tests pass against the mock program</li>
                <li>Use <code>Interface Analyzer</code> (Pydantic AI) to analyze <code>mock_program.py</code> and produce a structured description of its I/O interface:
                    <ul>
                        <li>Input/output methods (stdin, HTTP, CLI args, etc.)</li>
                        <li>Data formats (JSON, plain text, line-by-line, etc.)</li>
                        <li>Protocol details (ports, endpoints, encoding, etc.)</li>
                        <li>Concrete example interaction</li>
                    </ul>
                    This interface description is stored in state and passed to the Prototyper to ensure implementation matches the expected interface.
                </li>
            </ol>
        </li>
        <li><strong>Output:</strong> Complete E2E test harness in <code>e2e-tests/</code> directory containing <code>fixtures.json</code>, <code>run_tests.py</code>, <code>mock_program.py</code>, and <code>InterfaceDescription</code>.</li>
    </ul>

    <h4>Phase 1b: Project Scaffolding</h4>
    <ul>
        <li><strong>Cardinality:</strong> Runs <strong>once per project</strong>, after specification is complete.</li>
        <li><strong>Goal:</strong> Initialize a proper Python project structure using PyScaffold in the <code>prototype/</code> subdirectory.</li>
        <li><strong>Input:</strong> <code>ProjectMetadata</code> from the Analyst phase containing:
            <ul>
                <li><code>project_name</code>: Installable name (pip install name)</li>
                <li><code>package_name</code>: Python package name for imports</li>
                <li><code>description</code>: Short project description</li>
                <li><code>license</code>: License type (MIT, Apache-2.0, GPL-3.0-only, etc.)</li>
                <li><code>url</code>: Project URL (optional)</li>
                <li><code>github_actions</code>: Whether to add GitHub Actions CI config</li>
            </ul>
        </li>
        <li><strong>Process:</strong> Executes PyScaffold's <code>putup</code> command with the collected metadata to create a standard Python project structure including:
            <ul>
                <li><code>pyproject.toml</code> with project metadata</li>
                <li>Package directory structure</li>
                <li>Test directory</li>
                <li>Optional GitHub Actions CI configuration</li>
            </ul>
        </li>
        <li><strong>Mechanistic Check:</strong> Verifies that <code>putup</code> completes successfully. On failure, returns an error and halts.</li>
        <li><strong>Output:</strong> Initialized Python project structure in <code>prototype/</code> subdirectory, ready for the Prototyping phase to write code into.</li>
    </ul>

    <h4>Phase 2: The Prototyping Loop</h4>
    <ul>
        <li><strong>Cardinality:</strong> Runs <strong>once per project specification</strong>.</li>
        <li><strong>Agent:</strong> <code>Prototyper</code> (Anthropic Agent SDK).</li>
        <li><strong>Goal:</strong> Ground the specification in a functional, albeit rough, code implementation. This prototype serves as the "Oracle."</li>
        <li><strong>Input:</strong> <code>requirements.md</code> and <code>InterfaceDescription</code> from Phase 1a.</li>
        <li><strong>Process:</strong> The <code>Prototyper</code> writes a complete, monolithic Python script designed to fulfill the requirements, prioritizing functionality over code quality. The prompt includes the interface specification to ensure the prototype's I/O interface matches exactly what the E2E tests expect.</li>
        <li><strong>Mechanistic Check:</strong> <code>E2E Harness</code>.
            <ul>
                <li><strong>Trigger:</strong> The generated prototype fails any of the tests in <code>fixtures.json</code> or has syntax errors.</li>
                <li><strong>Feedback:</strong> Standard Python stack trace and explicit assertion failures from the E2E tests.</li>
                <li><strong>Action:</strong> The <code>Prototyper</code> is prompted to debug and correct its implementation.</li>
            </ul>
        </li>
        <li><strong>Output:</strong> <code>prototype.py</code> (a working but unrefined Python script).</li>
    </ul>

    <h4>Phase 3: The Refinement Loop</h4>
    <ul>
        <li><strong>Cardinality:</strong> Runs <strong>once per working prototype</strong>.</li>
        <li><strong>Agent:</strong> <code>Architect</code> (Anthropic Agent SDK).</li>
        <li><strong>Goal:</strong> Transform the monolithic prototype into a structurally clean architecture, adhering to "Functional Core, Imperative Shell" (FCIS) principles.</li>
        <li><strong>Input:</strong> <code>prototype.py</code>.</li>
        <li><strong>Process:</strong> The <code>Architect</code> refactors <code>prototype.py</code> by extracting pure logic into a designated <code>core</code> module and isolating side-effecting operations (I/O, network, database) into a <code>shell</code> module.</li>
        <li><strong>Mechanistic Check:</strong> <code>Architecture Reviewer</code> (Pydantic AI agent).
            <ul>
                <li><strong>Purpose:</strong> Validates that the refactored code adheres to FCIS principles.</li>
                <li><strong>Violation Detection:</strong>
                    <ul>
                        <li><strong>I/O in Core:</strong> Imports or calls to I/O-related modules (<code>os</code>, <code>sys</code>, <code>network</code>, <code>random</code>, <code>print</code>) detected within the <code>core</code> module.</li>
                        <li><strong>Forbidden Imports:</strong> Direct imports of external services, database connections, or file handles in core logic.</li>
                        <li><strong>Exceptions for Business Logic:</strong> Using exceptions for control flow instead of returning values.</li>
                        <li><strong>Missing Dependency Injection:</strong> Core functions that instantiate their own dependencies instead of receiving them as parameters.</li>
                        <li><strong>High Complexity in Shell:</strong> Significant business logic branching detected within <code>shell</code> functions.</li>
                    </ul>
                </li>
                <li><strong>Output:</strong> <code>ReviewerOutput</code> containing:
                    <ul>
                        <li><code>is_clean</code>: Boolean indicating whether architecture passes all checks</li>
                        <li><code>violations</code>: List of specific violations with file, line number, and description</li>
                        <li><code>summary</code>: Human-readable summary of the architectural review</li>
                    </ul>
                </li>
                <li><strong>Feedback:</strong> Specific architectural violation details (e.g., "Side effect found in <code>core.py</code> on line 45: <code>print()</code> call detected. Move to <code>shell</code>.").</li>
                <li><strong>Action:</strong> The <code>Architect</code> is prompted to correct the identified structural or impurity violations.</li>
            </ul>
        </li>
        <li><strong>Regression Check:</strong> After each refactoring pass, the E2E test suite is rerun against <code>fixtures.json</code> to ensure no behavioral regressions were introduced.</li>
        <li><strong>Output:</strong> A <code>src/</code> directory with <code>core/</code> and <code>shell/</code> modules and their respective files.</li>
    </ul>

    <h4>Phase 4: The Distillation Loop</h4>
    <ul>
        <li><strong>Cardinality:</strong> Runs <strong>once per refined prototype</strong>.</li>
        <li><strong>Goal:</strong> Decompose the entire refined prototype package into discrete, atomic units (symbols), topologically ordered for reconstruction via TDD.</li>
        <li><strong>Input:</strong> The architecturally refined code in <code>prototype/src/&lt;package&gt;/</code>.</li>
        <li><strong>Process:</strong>
            <ol>
                <li><strong>Inter-Module Dependency Graph:</strong> Uses <code>pydeps --show-deps --nodot</code> to analyze the package and build a module-level import graph.</li>
                <li><strong>Module Topological Sort:</strong> Sorts modules leaf-first (modules with no internal imports first, then modules whose dependencies have all been processed).</li>
                <li><strong>Intra-Module Symbol Analysis:</strong> For each module, uses Python's <code>ast</code> module to extract all top-level symbols:
                    <ul>
                        <li>Functions (<code>FunctionDef</code>, <code>AsyncFunctionDef</code>)</li>
                        <li>Classes (<code>ClassDef</code>)</li>
                        <li>Constants (<code>Assign</code>, <code>AnnAssign</code>)</li>
                        <li>Imports (<code>Import</code>, <code>ImportFrom</code>)</li>
                    </ul>
                    For each function/class, analyzes the body to identify dependencies (names used that are not parameters, locals, or builtins).
                </li>
                <li><strong>Symbol Topological Sort:</strong> Within each module, sorts symbols leaf-first based on internal dependencies.</li>
                <li><strong>Workspace Copy:</strong> Copies <code>prototype/</code> to <code>production/</code> to create a fresh workspace for the Ratchet cycle.</li>
            </ol>
        </li>
        <li><strong>Mechanistic Check:</strong> This phase is primarily deterministic. Errors in graph generation or AST parsing halt the system for human intervention.</li>
        <li><strong>Output:</strong> A prioritized <code>Unit Queue</code> of <code>UnitWorkItem</code> objects, each containing:
            <ul>
                <li><code>name</code>: Fully qualified name (e.g., <code>pkg.module.function</code>)</li>
                <li><code>symbol_type</code>: One of "function", "class", "constant", "import"</li>
                <li><code>module_path</code>: Absolute path to the source file</li>
                <li><code>line_number</code> / <code>end_line_number</code>: Location in file</li>
                <li><code>code</code>: Source code snippet</li>
                <li><code>dependencies</code>: List of local symbol names this unit depends on</li>
                <li><code>tests</code>: Empty list (populated by Oracle Generator in a future phase)</li>
            </ul>
        </li>
    </ul>

    <h4>Phase 4.5: Oracle Generator</h4>
    <ul>
        <li><strong>Cardinality:</strong> Runs <strong>once per unit</strong> (functions and classes only; skips constants and imports).</li>
        <li><strong>Agent:</strong> <code>Oracle</code> (Pydantic AI).</li>
        <li><strong>Goal:</strong> Analyze each unit from the prototype code and generate exhaustive test case specifications that will drive the TDD reconstruction.</li>
        <li><strong>Input:</strong> A <code>UnitWorkItem</code> containing the unit's source code, name, and dependencies.</li>
        <li><strong>Process:</strong> The Oracle agent performs deep analysis of the unit's code to understand:
            <ol>
                <li>The function/class signature and parameter types</li>
                <li>All possible execution paths and edge cases</li>
                <li>Expected behaviors for various inputs</li>
                <li>Error conditions and boundary cases</li>
            </ol>
        </li>
        <li><strong>Output:</strong> <code>OracleOutput</code> containing:
            <ul>
                <li><code>description</code>: An exhaustive description of what the unit does</li>
                <li><code>test_cases</code>: A list of <code>TestCaseDescription</code> objects, each with:
                    <ul>
                        <li><code>test_function_name</code>: Unique pytest-compatible function name</li>
                        <li><code>scenario</code>: Human-readable description of the test scenario</li>
                        <li><code>input_description</code>: Description of inputs to provide</li>
                        <li><code>expected_outcome</code>: What the test should verify</li>
                    </ul>
                </li>
            </ul>
        </li>
        <li><strong>Note:</strong> The Oracle populates the <code>tests</code> field in each <code>UnitWorkItem</code>, which is then consumed by the Ratchet Cycle.</li>
    </ul>

    <h3>3.2. Unit-Level Scope (1x per Unit)</h3>
    <p>This loop constructs and hardens individual units, transitioning through test execution and quality verification.</p>

    <h4>Step A: Unit Construction Iterator (Invokes Ratchet)</h4>
    <ul>
        <li><strong>Cardinality:</strong> Triggers the Ratchet Cycle <strong>once per test case</strong> within a unit's `Test Queue`.</li>
        <li><strong>Process:</strong> The system dequeues a unit from the `Unit Queue`, then iterates through its list of test definitions, feeding each one into the **Ratchet Cycle**.</li>
        <li><strong>Transition:</strong> Proceeds to Step B (Crucible) only after all tests for the current unit have been successfully implemented via the Ratchet Cycle (i.e., the unit is "functionally complete").</li>
    </ul>

    <h3>3.3. Atomic-Level Scope (1x per Test Case)</h3>
    <p>This is the innermost loop, focusing on building out functionality one rigorously tested step at a time.</p>

    <h4>Phase 5: The Ratchet Cycle</h4>
    <ul>
        <li><strong>Cardinality:</strong> Runs <strong>once per test case</strong> provided for a unit.</li>
        <li><strong>Goal:</strong> Implement minimal code to satisfy a single failing test, ensuring 100% coverage of new lines.</li>
        <li><strong>Input:</strong> A single <code>TestCaseDescription</code> from the Oracle Generator.</li>
        <li><strong>Process:</strong>
            <ol>
                <li><strong>Red Loop:</strong>
                    <ul>
                        <li><strong>Agent:</strong> <code>Tester</code> (Anthropic Agent SDK).</li>
                        <li><strong>Permission Enforcement:</strong> The Tester agent operates under strict permission constraints enforced via Claude Agent SDK <code>PreToolUse</code> hooks:
                            <ul>
                                <li>Can ONLY write to the <code>tests/</code> directory</li>
                                <li>CANNOT read the unit implementation file (prevents seeing the "answer")</li>
                                <li>CANNOT execute any tools (Bash, etc.) - the system runs tests externally</li>
                            </ul>
                        </li>
                        <li><strong>Action:</strong> Creates exactly one new test file/method that asserts the new behavior based only on the Oracle's specification.</li>
                        <li><strong>Mechanistic Check 1:</strong> <code>Test Validator</code> (Pydantic AI agent).
                            <ul>
                                <li><strong>Trigger:</strong> Immediately after test is written, before running pytest.</li>
                                <li><strong>Purpose:</strong> Validates that the newly written test correctly implements the Oracle's test case specification.</li>
                                <li><strong>Checks:</strong> Scenario matches, inputs are correct, expected outcome is verified.</li>
                                <li><strong>Feedback:</strong> If invalid, returns specific reason and prompts the Tester to fix.</li>
                            </ul>
                        </li>
                        <li><strong>Mechanistic Check 2:</strong> <code>Test Inventory Checker</code>.
                            <ul>
                                <li><strong>Trigger:</strong> If <code>pytest --collect-only</code> identifies <code>(new_tests_count != 1)</code> or if the newly added test passes against the current codebase.</li>
                                <li><strong>Feedback:</strong> "Error: You added N tests, expected 1. Remove extras." or "Error: The test `test_X` passed immediately; it must fail first."</li>
                                <li><strong>Action:</strong> The <code>Tester</code> is prompted to re-attempt creating the failing test.</li>
                            </ul>
                        </li>
                        <li><strong>Mechanistic Check 3:</strong> <code>Test Arbiter</code> (Pydantic AI agent).
                            <ul>
                                <li><strong>Trigger:</strong> Invoked when a test passes on the second attempt (i.e., should have failed but didn't).</li>
                                <li><strong>Purpose:</strong> Evaluates whether to keep or discard the test using Kent Beck's two criteria:
                                    <ol>
                                        <li><strong>Confidence:</strong> Does this test increase confidence in the system's correctness?</li>
                                        <li><strong>Communication:</strong> Does this test document a meaningfully different scenario?</li>
                                    </ol>
                                </li>
                                <li><strong>Decision:</strong>
                                    <ul>
                                        <li><strong>Keep:</strong> Test has high value (skip Green phase, commit directly)</li>
                                        <li><strong>Discard:</strong> Test has low value (rewind files and skip to next test case)</li>
                                    </ul>
                                </li>
                                <li><strong>Output:</strong> <code>ArbiterDecision</code> with <code>keep_test</code>, <code>confidence_value</code>, <code>communication_value</code>, and <code>reasoning</code>.</li>
                            </ul>
                        </li>
                    </ul>
                </li>
                <li><strong>Green Loop:</strong>
                    <ul>
                        <li><strong>Agent:</strong> <code>Developer</code> (Anthropic Agent SDK).</li>
                        <li><strong>Permission Enforcement:</strong> The Developer agent operates under strict permission constraints:
                            <ul>
                                <li>Can ONLY edit the specific unit implementation file</li>
                                <li>CANNOT read the test file (prevents seeing expected behavior)</li>
                                <li>CANNOT execute any tools - the system runs tests externally</li>
                            </ul>
                        </li>
                        <li><strong>Action:</strong> Writes the minimal implementation code required to make the newly added test pass.</li>
                        <li><strong>Mechanistic Check:</strong> <code>Coverage Intersection Checker</code>.
                            <ul>
                                <li><strong>Process:</strong>
                                    <ol>
                                        <li>Runs <code>pytest --cov --cov-report=json</code> to collect coverage data</li>
                                        <li>Extracts coverage for the specific unit's line range (from <code>line_number</code> to <code>end_line_number</code>)</li>
                                        <li>Computes intersection: lines within the function that are NOT executed</li>
                                    </ol>
                                </li>
                                <li><strong>Trigger:</strong> If any tests fail, OR if there are uncovered lines within the unit's function body (dead code).</li>
                                <li><strong>Feedback:</strong> "Error: Test failed: [Stack Trace]" or "Error: Dead code detected on lines [Y, Z] within the function. Remove unreachable code or ensure tests exercise all paths."</li>
                                <li><strong>Baseline Persistence:</strong> On success, saves coverage baseline to <code>.breakfix/coverage/{fqn}.json</code> for regression detection in future iterations.</li>
                                <li><strong>Action:</strong> The <code>Developer</code> is prompted to correct the implementation.</li>
                            </ul>
                        </li>
                    </ul>
                </li>
            </ol>
        </li>
        <li><strong>File Checkpointing:</strong> Both Red and Green agents use Claude Agent SDK's file checkpointing (<code>enable_file_checkpointing=True</code>) to enable atomic rollback. If an agent exceeds max retries, the system rewinds all file changes to the checkpoint before the agent started.</li>
        <li><strong>Output:</strong> The committed, functionally added / modified code for the unit. Control returns to the Unit Construction Iterator.</li>
    </ul>

    <h2>4. Phase 7: The Crucible (Hardening & Polishing)</h2>
    <p>This phase is paramount for ensuring the production readiness of each unit. It runs **once per unit** after its construction via the Ratchet Cycle is complete.</p>

    <h4>Sub-Cycle A: Mutation Testing (The Hardening)</h4>
    <ul>
        <li><strong>Cardinality:</strong> Runs <strong>once per unit</strong>, potentially iterating internally if mutants survive.</li>
        <li><strong>Goal:</strong> Verify the completeness and sensitivity of the unit's test suite.</li>
        <li><strong>Input:</strong> The functionally complete unit code and its corresponding test suite.</li>
        <li><strong>Process:</strong>
            <ol>
                <li><strong>Mutant Generator:</strong> Invokes a mutation testing tool (e.g., <code>mutmut</code>, <code>cosmic-ray</code>) to automatically introduce small, logic-altering changes (mutants) into the unit's source code.</li>
                <li><strong>Mutant Execution:</strong> The unit's test suite is run against each mutant.</li>
                <li><strong>Mechanistic Check:</strong> <code>Mutant Status Checker</code>.
                    <ul>
                        <li><strong>Trigger:</strong> A mutant <strong>survives</strong> (i.e., the test suite passes on the mutated code, implying the tests are not sensitive enough to detect the logic change).</li>
                        <li><strong>Action (Agent Triggered):</strong> The <code>Sentinel</code> (Anthropic Agent SDK) is alerted.</li>
                    </ul>
                </li>
                <li><strong>Sentinel Loop:</strong>
                    <ul>
                        <li><strong>Agent:</strong> <code>Sentinel</code> (Anthropic Agent SDK).</li>
                        <li><strong>Action:</strong> Receives the diff of the surviving mutant and is tasked with generating a *new* test case that passes on the original code but fails on the mutant.</li>
                        <li><strong>Mechanistic Check:</strong> <code>Mutant Killer Verifier</code>.
                            <ul>
                                <li><strong>Trigger:</strong> The proposed test either fails against the *original* code or passes against the *mutant* code.</li>
                                <li><strong>Feedback:</strong> "Error: Proposed test `test_killer` failed on original code (regression for original)" or "Error: Proposed test `test_killer` failed to kill the mutant; it still passed."</li>
                                <li><strong>Action:</strong> The <code>Sentinel</code> is prompted to re-attempt writing a killer test. If retries are exhausted, the mutant may be flagged as "equivalent" or require human intervention.</li>
                            </ul>
                        </li>
                    </ul>
                </li>
            </ol>
        </li>
        <li><strong>Output:</strong> A highly robust, "mutation-pure" test suite for the unit (100% mutation score).</li>
    </ul>

    <h4>Sub-Cycle B: Optimization (The Polishing)</h4>
    <ul>
        <li><strong>Cardinality:</strong> Runs <strong>once per unit</strong>, potentially iterating internally for multiple optimization passes.</li>
        <li><strong>Goal:</strong> Refactor the unit's code to improve readability, performance, and adherence to best practices, with complete confidence in the test suite.</li>
        <li><strong>Input:</strong> The unit's code and its 100% mutation-score-verified test suite (now a impenetrable safety net).</li>
        <li><strong>Agent:</strong> <code>Optimizer</code> (Anthropic Agent SDK).</li>
        <li><strong>Process:</strong>
            <ol>
                <li>The <code>Optimizer</code> receives the unit's code and a prompt instructing it to apply improvements without changing behavior.</li>
                <li><strong>Mechanistic Check:</strong> <code>Regression Runner</code>.
                    <ul>
                        <li><strong>Trigger:</strong> Any modification by the <code>Optimizer</code> causes any test in the hardened suite to fail.</li>
                        <li><strong>Feedback:</strong> Detailed stack trace and specific test failure reports.</li>
                        <li><strong>Action:</strong> The <code>Optimizer</code> is prompted to fix the regression or revert its changes.</li>
                    </ul>
                </li>
                <li><strong>Convergence Check:</strong> The system monitors if the <code>Optimizer</code> continues to propose meaningful improvements.</li>
            </ol>
        </li>
        <li><strong>Output:</strong> The final, optimized, and regression-free unit code. Control returns to the `Unit Scope` to fetch the next unit.</li>
    </ul>

</body>
</html>

							<script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
							<script>
								mermaid.initialize({ startOnLoad: true });
							</script>
                        </body>
                        </html>
                    
